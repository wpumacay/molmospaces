{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8421ed1",
   "metadata": {},
   "source": [
    "**Evaluation → CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tempfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "THOR_CAT_SIMPLIFY = {\n",
    "    \"saltshaker\": \"S/P Shaker\", \"peppershaker\": \"S/P Shaker\",\n",
    "    \"tomato\": \"Fruit\", \"apple\": \"Fruit\",\n",
    "    \"butterknife\": \"Knife\", \"boiler\": \"Kettle\",\n",
    "    \"winebottle\": \"Bottle\", \"atomizer\": \"Spray Bottle\",\n",
    "    \"remotecontrol\": \"Remote Control\", \"soapdispenser\": \"Soap Dispenser\",\n",
    "    \"tissuepaper\": \"Tissue Paper\",\n",
    "}\n",
    "\n",
    "\n",
    "def _extract_object_name(obs_scene_bytes):\n",
    "    try:\n",
    "        obs = json.loads(obs_scene_bytes.decode(\"utf-8\"))\n",
    "        raw = obs.get(\"object_name\", \"unknown\")\n",
    "        cleaned = \"\".join(c if c.isalpha() else \" \" for c in raw).strip()\n",
    "        return cleaned.split()[0] if cleaned else \"unknown\"\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def _simplify(name: str) -> str:\n",
    "    simp = THOR_CAT_SIMPLIFY.get(name.lower(), name)\n",
    "    return \" \".join(w.capitalize() for w in simp.split())\n",
    "\n",
    "\n",
    "def _bayesian_ci(successes, total, alpha=0.05):\n",
    "    if total == 0:\n",
    "        return 0.0, 0.0\n",
    "    a, b = 1 + successes, 1 + (total - successes)\n",
    "    return beta_dist.ppf(alpha / 2, a, b) * 100, beta_dist.ppf(1 - alpha / 2, a, b) * 100\n",
    "\n",
    "\n",
    "def _copy_group(src, dst):\n",
    "    for k, item in src.items():\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            dst.create_dataset(k, data=item[()])\n",
    "        elif isinstance(item, h5py.Group):\n",
    "            _copy_group(item, dst.create_group(k))\n",
    "\n",
    "\n",
    "def _decode_json_sequence(raw_uint8):\n",
    "    rows = []\n",
    "    for row in raw_uint8:\n",
    "        d = json.loads(bytes(row).rstrip(b\"\\x00\").decode(\"utf-8\"))\n",
    "        flat = []\n",
    "        for v in d.values():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                flat.extend(v)\n",
    "            else:\n",
    "                flat.append(v)\n",
    "        rows.append(flat)\n",
    "    return np.array(rows, dtype=np.float64)\n",
    "\n",
    "\n",
    "def _episode_joint_jerk(ep, dt, max_steps=None):\n",
    "    raw_q = None\n",
    "    try:\n",
    "        raw_q = ep[\"obs\"][\"agent\"][\"qpos\"][:]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            raw_q = ep[\"actions\"][\"joint_pos\"][:]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if raw_q is None:\n",
    "        return np.nan\n",
    "    if max_steps is not None:\n",
    "        raw_q = raw_q[:max_steps]\n",
    "    q = _decode_json_sequence(raw_q)\n",
    "    if q.shape[0] < 4:\n",
    "        return np.nan\n",
    "    d3 = q[3:] - 3 * q[2:-1] + 3 * q[1:-2] - q[:-3]\n",
    "    d3 /= dt ** 3\n",
    "    return float(np.mean(np.linalg.norm(d3, axis=1)))\n",
    "\n",
    "\n",
    "def _combine_trajectories(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    h5_files = sorted(folder.rglob(\"*.h5\"))\n",
    "    if not h5_files:\n",
    "        raise FileNotFoundError(f\"No .h5 found under {folder_path}\")\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix=\".h5\", delete=False)\n",
    "    tmp.close()\n",
    "    out = h5py.File(tmp.name, \"w\")\n",
    "    ep = 0\n",
    "    for src_path in h5_files:\n",
    "        try:\n",
    "            src = h5py.File(src_path, \"r\")\n",
    "            for tk in [k for k in src.keys() if k.startswith(\"traj_\")]:\n",
    "                dst = out.create_group(f\"episode_{ep:04d}_{tk}\")\n",
    "                _copy_group(src[tk], dst)\n",
    "                ep += 1\n",
    "            src.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: skipping {src_path}: {e}\")\n",
    "    out.close()\n",
    "    print(f\"Combined {ep} episodes from {len(h5_files)} files → {tmp.name}\")\n",
    "    return tmp.name\n",
    "\n",
    "\n",
    "def eval_to_csv(\n",
    "    run_path: str,\n",
    "    policy_name: str,\n",
    "    reward_threshold: float | None = 0.01,\n",
    "    output_csv: str = \"eval_results.csv\",\n",
    "    dt: float = 0.1,\n",
    "    number_steps_per_episode: int | None = 450\n",
    "):\n",
    "\n",
    "    combined_h5 = _combine_trajectories(run_path)\n",
    "    per_obj = defaultdict(lambda: {\"success\": 0, \"total\": 0, \"jerk_joint\": []})\n",
    "    total_s, total_n = 0, 0\n",
    "    all_jerk_joint = []\n",
    "\n",
    "    with h5py.File(combined_h5, \"r\") as f:\n",
    "        for key in sorted(f.keys()):\n",
    "            if not key.startswith(\"episode_\"):\n",
    "                continue\n",
    "            ep = f[key]\n",
    "\n",
    "            if reward_threshold is not None and \"rewards\" in ep:\n",
    "                r = ep[\"rewards\"][:]\n",
    "                if number_steps_per_episode is not None:\n",
    "                    r = r[:number_steps_per_episode]\n",
    "                success = r.size > 0 and float(r.max()) >= reward_threshold\n",
    "            elif \"success\" in ep:\n",
    "                s_arr = ep[\"success\"][:]\n",
    "                if number_steps_per_episode is not None:\n",
    "                    s_arr = s_arr[:number_steps_per_episode]\n",
    "                success = bool(max(s_arr)) if len(s_arr) > 0 else False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            jj = _episode_joint_jerk(ep, dt, max_steps=number_steps_per_episode)\n",
    "            obj = _simplify(_extract_object_name(ep[\"obs_scene\"][()])) if \"obs_scene\" in ep else \"Unknown\"\n",
    "\n",
    "            per_obj[obj][\"total\"] += 1\n",
    "            per_obj[obj][\"success\"] += int(success)\n",
    "            if not np.isnan(jj):\n",
    "                per_obj[obj][\"jerk_joint\"].append(jj)\n",
    "                all_jerk_joint.append(jj)\n",
    "\n",
    "            total_n += 1\n",
    "            total_s += int(success)\n",
    "\n",
    "    # build rows\n",
    "    rows = []\n",
    "    for obj in sorted(per_obj):\n",
    "        d = per_obj[obj]\n",
    "        s, t = d[\"success\"], d[\"total\"]\n",
    "        rate = 100.0 * s / t if t else 0.0\n",
    "        ci_lo, ci_hi = _bayesian_ci(s, t)\n",
    "        mean_jj = float(np.mean(d[\"jerk_joint\"])) if d[\"jerk_joint\"] else np.nan\n",
    "        std_jj = float(np.std(d[\"jerk_joint\"])) if d[\"jerk_joint\"] else np.nan\n",
    "        rows.append(dict(\n",
    "            policy=policy_name, category=obj, successes=s, total=t,\n",
    "            success_rate_pct=round(rate, 2),\n",
    "            ci_95_low_pct=round(ci_lo, 2), ci_95_high_pct=round(ci_hi, 2),\n",
    "            jerk_joint_mean=round(mean_jj, 6) if not np.isnan(mean_jj) else np.nan,\n",
    "            jerk_joint_std=round(std_jj, 6) if not np.isnan(std_jj) else np.nan,\n",
    "        ))\n",
    "\n",
    "    rate = 100.0 * total_s / total_n if total_n else 0.0\n",
    "    ci_lo, ci_hi = _bayesian_ci(total_s, total_n)\n",
    "    mean_jj = float(np.mean(all_jerk_joint)) if all_jerk_joint else np.nan\n",
    "    std_jj = float(np.std(all_jerk_joint)) if all_jerk_joint else np.nan\n",
    "    rows.append(dict(\n",
    "        policy=policy_name, category=\"OVERALL\", successes=total_s, total=total_n,\n",
    "        success_rate_pct=round(rate, 2),\n",
    "        ci_95_low_pct=round(ci_lo, 2), ci_95_high_pct=round(ci_hi, 2),\n",
    "        jerk_joint_mean=round(mean_jj, 6) if not np.isnan(mean_jj) else np.nan,\n",
    "        jerk_joint_std=round(std_jj, 6) if not np.isnan(std_jj) else np.nan,\n",
    "    ))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    with open(output_csv, \"w\") as fout:\n",
    "        fout.write(f\"# policy_name: {policy_name}\\n\")\n",
    "        fout.write(f\"# run_path: {run_path}\\n\")\n",
    "        fout.write(f\"# reward_threshold: {reward_threshold}\\n\")\n",
    "        fout.write(f\"# dt: {dt}\\n\")\n",
    "        fout.write(f\"# number_steps_per_episode: {number_steps_per_episode}\\n\")\n",
    "        df.to_csv(fout, index=False)\n",
    "\n",
    "    print(f\"\\nSaved → {os.path.abspath(output_csv)}\")\n",
    "\n",
    "    os.unlink(combined_h5)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PATH = \"/home/orayyan/projects/molmospaces/eval_output/new_results/open/pi05\"\n",
    "\n",
    "df = eval_to_csv(RUN_PATH, policy_name=\"pi05\", reward_threshold=0.15, output_csv=\"pi05.csv\", dt=0.1, number_steps_per_episode=450)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
